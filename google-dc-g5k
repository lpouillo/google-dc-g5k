#!/usr/bin/env python

import json
import argparse
import requests
import math
from os import fdopen

from string import Template
from tempfile import mkstemp
from itertools import cycle
from logging import INFO, DEBUG, WARN
from pprint import pformat
from execo import Remote, TaktukRemote, logger, Process, SshProcess, configuration, Put
from execo.log import style
from execo_g5k import get_g5k_sites, get_current_oar_jobs, get_planning,\
    compute_slots, find_free_slot, distribute_hosts, get_jobs_specs, \
    oarsub, get_oar_job_info, wait_oar_job_start, get_oar_job_nodes, \
    get_oar_job_subnets, Deployment, deploy, get_host_site, \
    default_frontend_connection_params, get_g5k_clusters, get_host_attributes, \
    get_site_clusters, get_resource_attributes
from execo_g5k.planning import show_resources
from execo.time_utils import format_date
from vm5k.utils import hosts_list, get_CPU_RAM_FLOPS


default_job_name = 'GoogleDataCenter'
default_walltime  = '2:00:00'
default_n_vnodes = 500
default_vnodes_cpu = {"vcores": [{"id": "0", "pcore": "0", "frequency": "1000"}]}
default_vnodes_mem = {"mem": "256 Mo", "swap": "256 Mo"}

default_root_fs = 'file:///home/ejeanvoine/public/distem/distem-fs-wheezy.tar.gz'

configuration['color_styles']['step'] = 'on_yellow', 'bold'

def main():
    """
    
    """
    logger.info('%s\n', style.step(' Playing with a Google Datacenter on Grid\'5000'))
    args = set_options()
    logger.info(style.step('Retrieve Grid\'5000 resources'))
    hosts, vnet = get_resources(args.vnodes, args.vnodes_mem, args.walltime, args.job_name)
    if not vnet or not hosts:
        logger.error('Error in job resources')
        exit()
    if args.action == 'deploy':
        logger.info(style.step('Configure distem on physical hosts'))    
        logger.debug('max_iface=%s', math.ceil(args.vnodes / len(hosts)))
        coordinator = setup_distem(hosts, vnet,
                                   max_iface=int(math.ceil(args.vnodes / len(hosts))))
        if not coordinator:
            logger.error('Problem with distem installation')
            exit()
        logger.info(style.step('Create virtual nodes'))
        init_vnodes(coordinator, args.vnodes, args.vnodes_mem, args.vnodes_cpu)
    
    elif args.actions == 'play':
        logger.error('Not Implemented')
        

def set_options():
    """ """
    parser = argparse.ArgumentParser(description="Install OpenCloudWare on G5K")
    optio = parser.add_mutually_exclusive_group()
    optio.add_argument("-v", "--verbose",
                       action="store_true",
                       help='print debug messages')
    optio.add_argument("-q", "--quiet",
                       action="store_true",
                       help='print only warning and error messages')
    parser.add_argument('action', 
                        help='Action to be done (deploy, play)')
    parser.add_argument("-j", "--job-name",
                        default=default_job_name,
                        help="name of the OAR job")
    parser.add_argument("-n", "--vnodes",
                        type=int,
                        default=default_n_vnodes,
                        help="number of virtual nodes to be created")
    parser.add_argument("-c", "--vnodes-cpu",
                        type=int,
                        default=default_vnodes_cpu,
                        help="number of virtual cpu for the virtual nodes")
    parser.add_argument("-m", "--vnodes-mem",
                        type=int,
                        default=default_vnodes_mem,
                        help="size in Mb of the virtual nodes memory")
    parser.add_argument("-w", "--walltime",
                        default=default_walltime,
                        help="Walltime for the OAR job")
    args = parser.parse_args()
    if args.verbose:
        logger.setLevel(DEBUG)
    elif args.quiet:
        logger.setLevel(WARN)
    else:
        logger.setLevel(INFO)
    return args


def get_resources(vnodes=None, vnodes_mem=None, walltime=None, job_name=None):
    """Try to find a running job and reserve resources if none found"""
    logger.info('Looking for a running job ...')
    job_id = None
    running_jobs = get_current_oar_jobs(get_g5k_sites())
    for job in running_jobs:
        info = get_oar_job_info(job[0], job[1])
        if info['name'] == job_name:
            job_id = job[0]
            site = job[1]
            logger.info('Job %s found on site %s!', style.emph(job_id),
                        style.host(site))
            break

    if not job_id:
        logger.info('None found, performing a new reservation')
        job_id, site = _make_reservation(vnodes, vnodes_mem, walltime, job_name)
        if not job_id:
            return None, None
    
    logger.info('Waiting for job start ...')
    wait_oar_job_start(job_id, site)
    job_info =  get_resource_attributes('/sites/' + site + 
                                  '/jobs/' + str(job_id))
    hosts = job_info['assigned_nodes']
    logger.info('Hosts: %s', hosts_list(hosts))
    vnets = job_info['resources_by_type']['subnets']
    mask = 22 - int(math.ceil(math.log(len(vnets), 2)))
    vnet = vnets[0].replace('/22', '/' + str(mask))
    logger.info('Virtual Network(s): %s', vnet)

    return hosts, vnet


def setup_distem(hosts, vnet, max_iface):
    """Deploy the hosts with a NFS environment and setup distem with
    the given vnet.
    
    Return:
        coordinator
    """
    logger.info('Deploying hosts')
    deployed_hosts, _ = deploy(Deployment(hosts=hosts,
                                          env_name="wheezy-x64-nfs"))
    hosts = sorted(list(deployed_hosts), key=lambda host: (host.split('.', 1)[0].split('-')[0],
                                    int(host.split('.', 1)[0].split('-')[1])))
    if len(hosts) == 0:
        logger.error('No nodes deployed !')
        return None
    
    
    fd, nodes_file = mkstemp(dir='/tmp/', prefix='distem_nodes_')
    f = fdopen(fd, 'w')
    f.write('\n'.join(hosts))
    f.close()
    
    scan_hosts = Process('nmap -v -oG - -i ' + nodes_file + ' -p 4567 | grep open')
    scan_hosts.shell = True
    scan_hosts.nolog_exit_code = scan_hosts.ignore_exit_code = True
    scan_hosts.run()
    
    if scan_hosts.stdout:
        coordinator = scan_hosts.stdout[scan_hosts.stdout.find('(') + 1:
                                        scan_hosts.stdout.find(')')]
        logger.info('Existing instance found on %s, cleaning ..', 
                    style.host(coordinator))
        rest_url = "http://" + coordinator + ':4567'
        r = requests.get(rest_url + '/vnodes/?')
        for vnode in json.loads(r.content):
            logger.detail('Destroying %s', vnode['name'])
            r = requests.put(rest_url + '/vnodes/' + vnode['name'], 
                              data={'type': 'remove'})
        logger.info('Done')
        exit()
    else:
        coordinator = hosts[0]
        logger.info('Performing distem bootstrap ...')
        Put(get_host_site(coordinator), [nodes_file], remote_location='/tmp/',
        connection_params={'user': default_frontend_connection_params['user']}).run()
        distem_install = SshProcess('distem-bootstrap --max-vifaces ' + str(max_iface) + 
                                    '  -f ' + nodes_file + ' -d -D --verbose',
                                    get_host_site(coordinator),
                                    connection_params={'user': default_frontend_connection_params['user']}).run()
        if not distem_install.ok:
            logger.error('Error in installing distem \n%s', distem_install.stdout)
            return None
        distem_vnet = SshProcess('distem --coordinator host=%s '
                                 '--create-vnetwork vnetwork=vnetwork,address=%s'
                                 % (coordinator, vnet),
                                    coordinator).run()
                                    
        logger.info('Distem is ready to be used on %s', style.emph(coordinator))
    
    return coordinator

def init_vnodes(coordinator, n_nodes=None, vmem=None, vcpu=None):
    """ """
    rest_url = "http://" + coordinator + ":4567"
    r = requests.get(rest_url + '/pnodes/')
    pnodes = json.loads(r.content)
    hosts = map(lambda x: x['address'], pnodes)
    logger.info('Create the %s vnodes', style.emph(n_nodes))
    n_by_host = int(n_nodes / len(hosts))

    
    vnodes = [{"id": "0",
              "name": "node-" + str(i * n_by_host + j + 1),
              "status": "INIT",
              "vfilesystem": {"image": "file/home/ejeanvoine/public/distem/distem-fs-wheezy.tar.gz",
                              "shared": False,
                              "path": None,
                              "sharedpath": None,
                              "cow": False},
               "vifaces": [{"name": "if0",
                            "vnetwork": "vnetwork"}],
               "vcpu": default_vnodes_cpu,
               "vmem": default_vnodes_mem,
               "host": host,
               "ssh_key": None} 
              for i, host in enumerate(hosts)
              for j in range(n_by_host + 1 )]
     
    
    from pprint import pprint
    for vnode in vnodes:
        r = requests.post(rest_url + '/vnodes/' + vnode['name'], 
                          data={"desc": json.dumps(vnode)})

    exit()
    
    base_cmd = Template('distem --create-vnode vnode=node-$i_node '
                       'rootfs=file:///home/ejeanvoine/public/distem/distem-fs-wheezy.tar.gz ; '
                       'distem --create-viface vnode=node-$i_node,iface=if0,vnetwork=vnetwork ; '
                       'distem --start-vnode node-$i_node')
    cmds = [base_cmd.substitute(i_node=i * n_by_host + j + 1) 
            for i, host in enumerate(hosts)
            for j in range(n_by_host)]
    chunk_size = 30
    # Distem is saturating around 75 parallel request so we split
    for chunk in [cmds[x : x + chunk_size] for x in xrange(0, len(cmds), chunk_size)]:
        TaktukRemote('{{chunk}}', [coordinator] * len(chunk)).run()
        logger.detail('%s vnodes have been started', chunk_size)
    
    
    (resp_headers, content) = h.request(rest_url +"/vnodes/?", "GET")
    vnodes = sorted(json.loads(content), key=lambda n: n['name'].split('-')[0])
    f = open('nodes.list', 'w')
    f.write('\n'.join(map(lambda n: n['vifaces'][0]['address'].split('/')[0] 
                          + '\t' + n['name'],
                          vnodes)) + '\n')
    f.close()
    
    logger.info('All vnodes are ready to be used, see %s', style.emph('nodes.list'))
        
    return vnodes


def _check_reservation(vnodes=None, vnodes_mem=None, job_id=None, site=None):
    """ """
    

def _make_reservation(vnodes=None, vnodes_mem=None, walltime=None, job_name=None):
    """ """
    # find the first slot when a combination of resources on one site has
    # enough memory
    required_mem = vnodes_mem * vnodes * 10 ** 6
    blacklisted = ['sagittaire']
    logger.info('Looking for a slot that can sustain required memory: %s Gb', 
                style.emph(required_mem / 10 ** 9))
    
    planning = get_planning(['grid5000'], subnet=True)
    slots = compute_slots(planning, walltime=walltime,
                          excluded_elements=blacklisted)
    clusters_mem = {cluster: get_host_attributes(cluster + '-1')['main_memory']['ram_size']
                    for cluster in get_g5k_clusters()}
    slot_ok = False
    for startdate, _, res in slots:
        for site in get_g5k_sites():
            site_res = {k: res[k] for k in get_site_clusters(site)
                        if k in res and k not in blacklisted}
            mem_available = sum(value * clusters_mem[key] 
                                for key, value in site_res.iteritems())            
            if mem_available > required_mem:
                slot_ok = True
                clusters = [cluster for cluster in get_site_clusters(site)
                            if cluster not in blacklisted]  
                break
        if slot_ok == True:
            break
    if not slot_ok:
        logger.error('Unable to find a slot for your deployment')
        return None

    # compute the number of hosts needed
    resources_needed = {}
    resources_available = site_res
    logger.debug('resources available' + pformat(resources_available))
    iter_clusters = cycle(clusters)
    while required_mem > 0:
        cluster = iter_clusters.next()
        if resources_available[cluster] == 0:
            clusters.remove(cluster)
            iter_clusters = cycle(clusters)
        else:
            resources_available[cluster] -= 1
            required_mem -= clusters_mem[cluster]

            if cluster not in resources_needed:
                resources_needed[cluster] = 0
            resources_needed[cluster] += 1
                
    jobs_specs = get_jobs_specs(resources_needed, name=job_name,
                                excluded_elements=blacklisted)
    mask = min(22, 32 - int(math.ceil(math.log(vnodes + 2, 2))))
    
    sub, site = jobs_specs[0]
    sub.resources = 'slash_' +str(mask) +'=1+' + sub.resources
    sub.walltime = walltime
    sub.additional_options = "-t deploy"
    sub.reservation_date = startdate

    jobs = oarsub([(sub, site)])
    job_id = jobs[0][0]
    logger.info('Job %s will start at %s on %s', style.emph(job_id),
                style.log_header(format_date(startdate)),
                style.host(site))
    
    return job_id, site



    


if __name__ == "__main__":
    main()
